{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of torch2tflite.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO/hhZWHE9l3FYQtqoSy9rP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fTB--2B1-ziI"},"source":["import torch\n","import torchvision\n","from torchvision.models.segmentation import segmentation\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import time\n","import matplotlib.image as mpimg\n","import cv2\n","import tensorflow as tf\n","from torchvision import transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-Lhzi1Yi-7U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633082468335,"user_tz":-600,"elapsed":36751,"user":{"displayName":"Minsoo Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06852921966570921891"}},"outputId":"b0c27aa2-2ea1-4d8c-8827-21d191b94ae5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0P2vSadvi_iD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633082468336,"user_tz":-600,"elapsed":10,"user":{"displayName":"Minsoo Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06852921966570921891"}},"outputId":"8b12d27d-eb79-4781-9570-c2482c444cf7"},"source":["cd drive/MyDrive/AI_TfLite/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AI_TfLite\n"]}]},{"cell_type":"code","metadata":{"id":"D7ShOKv7L_9k"},"source":["#Set device to GPU_indx if GPU is avaliable\n","GPU_indx = 0\n","device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7d7yyjdMvyyx"},"source":["# Loading the PyTorch Model"]},{"cell_type":"code","metadata":{"id":"vOfFNqIvjEqV"},"source":["net = segmentation.lraspp_mobilenet_v3_large(pretrained=False, progress=False, num_classes=3).to(device)\n","print(net)\n","Pretrained_Path = \"./bdd_city_320_mapil_w.pt\"\n","check_point = torch.load(Pretrained_Path)\n","net.load_state_dict(check_point['model_state_dict'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBrAof7sNdDZ"},"source":["# Requires shape of the image it has been used to train with\n","\n","x = np.load('val.npz', mmap_mode='r')\n","image = x['arr_0']\n","label = x['arr_1']\n","\n","i=140\n","ground_image=image[i][:][:][:]\n","ground_label=label[i][:][:][:]\n","\n","ground_label=np.squeeze(ground_label)\n","\n","processed_img_t=np.moveaxis(ground_image,-1,0)\n","processed_img_t=torch.tensor(processed_img_t).unsqueeze(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRX6EkaSwH78"},"source":["# Converting PyTorch model to ONNX model"]},{"cell_type":"code","metadata":{"id":"7fAwRL6LL1Xj"},"source":["# Export the model from PyTorch to ONNX\n","torch_out = torch.onnx._export(net,             # model being run\n","                                processed_img_t.to(device),          # model input (or a tuple for multiple inputs)\n","                                \"LRASPP.onnx\",      # where to save the model (can be a file or file-like object)\n","                                export_params=True,       # store the trained parameter weights inside the model file\n","                                input_names=['input'],     # specify the name of input layer in onnx model\n","                                output_names=['output'])     # specify the name of input layer in onnx model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JW-zEviCwQqu"},"source":["# Converting ONNX model to Tensorflow model"]},{"cell_type":"code","metadata":{"id":"RtdjiwUmTGLX"},"source":["import onnx\n","from onnx_tf.backend import prepare"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXjSO-ujUGtA"},"source":["!onnx-tf convert -i \"LRASPP.onnx\" -o  \"LRASPP.pb\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZR133eYwVuB"},"source":["## Converting Tensorflow model to TFLite model"]},{"cell_type":"code","metadata":{"id":"zDQSg-dnUaC9"},"source":["converter =  tf.compat.v1.lite.TFLiteConverter.from_saved_model(\"LRASPP.pb/\")\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model = converter.convert()\n","with tf.io.gfile.GFile('LRASPP.tflite', 'wb') as f:\n","  f.write(tflite_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlxSE7-oUnPI"},"source":["x = np.load('val.npz', mmap_mode='r')\n","image = x['arr_0']\n","label = x['arr_1']\n","\n","i=141\n","ground_image=image[i][:][:][:]\n","ground_label=label[i][:][:][:]\n","\n","ground_label=np.squeeze(ground_label)\n","\n","processed_img_t=np.moveaxis(ground_image,-1,0)\n","processed_img_t=torch.tensor(processed_img_t).unsqueeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPfoa1fVwoQn"},"source":["# Infering TFLite model"]},{"cell_type":"code","metadata":{"id":"XhWl0y2BWSDP"},"source":["# Load TFLite model and allocate tensors.\n","interpreter = tf.lite.Interpreter(model_path=\"LRASPP.tflite\")\n","interpreter.allocate_tensors()\n","\n","# Get input and output tensors.\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","# Test model on random input data.\n","input_shape = input_details[0]['shape']\n","\n","\n","input_data = processed_img_t.numpy()\n","print(input_data.shape)\n","# print(input_data.shape)\n","interpreter.set_tensor(input_details[0]['index'], input_data)\n","print(input_details)\n","\n","start_time = time.time()\n","\n","interpreter.invoke()\n","\n","# The function `get_tensor()` returns a copy of the tensor data.\n","# Use `tensor()` in order to get a pointer to the tensor.\n","output_data = interpreter.get_tensor(output_details[0]['index'])\n","output_data = np.argmax(output_data.squeeze(), axis=0)\n","end_time = time.time() - start_time\n","\n","\n","actual_img = input_data.squeeze()\n","actual_img = actual_img.transpose(1,2,0)\n","plt.figure(figsize=(15,15))\n","plt.imshow(actual_img)\n","\n","image = output_data\n","plt.figure(figsize=(15,15))\n","plt.imshow(image)\n","\n","print(end_time)"],"execution_count":null,"outputs":[]}]}